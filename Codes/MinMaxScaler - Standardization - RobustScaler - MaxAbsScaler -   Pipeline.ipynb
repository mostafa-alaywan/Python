{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Data normalization with Pandas and Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**https://towardsdatascience.com/data-normalization-with-pandas-and-scikit-learn-7c1cc6ed6475**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The success of a machine learning algorithm highly depends on the quality of the data fed into the model. Real-world data is often dirty containing outliers, missing values, wrong data types, irrelevant features, or non-standardized data. The presence of any of these will prevent the machine learning model to properly learn. For this reason, transforming raw data into a useful format is an essential stage in the machine learning process. One technique you will come across multiple times when pre-processing data is normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Normalization is a common practice in machine learning which consists of transforming numeric columns to a common scale. In machine learning, some feature values differ from others multiple times. The features with higher values will dominate the learning process. However, it does not mean those variables are more important to predict the outcome of the model. Data normalization transforms multiscaled data to the same scale. After normalization, all variables have a similar influence on the model, improving the stability and performance of the learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are multiple normalization techniques we will cover the most important:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. The maximum absolute scaling**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. The min-max feature scaling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. The z-score method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. The robust scaling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following data frame contains the inputs (independent variables) of a multiple regression model for predicting the price of a second-hand car: (1) the odometer reading (km) and (2) the fuel economy (km/l). In this article, we use a small data set for learning purposes. However, in the real world, the data sets employed will be much larger.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from plotnine import ggplot , aes , geom_point, theme_minimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data frame containing the odometer reading (km) and the fuel economy (km/l) of second-hand cars\n",
    "df_cars = pd.DataFrame([[120000, 11], [250000, 11.5], [175000, 15.8], [350000, 17], [400000, 10]],\n",
    "                       columns=['odometer_reading', 'fuel_economy'])\n",
    "df_cars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This short code example creates a plot from the df_cars dataset. Here’s a quick breakdown:\n",
    "\n",
    "\n",
    "1. we import the **ggplot()** class as well as some useful functions from **plotnine**, **aes()** , **geom_point()** and **theme_minimal().**\n",
    "\n",
    "2. we create a plot object using **ggplot()**, passing the **df_cars** DataFrame to the constructor.\n",
    "\n",
    "3. we add **aes()** to set the variable to use for each axis, in this case **odometer_reading** and **fuel_economy.**\n",
    "\n",
    "4. we add **geom_point()** to specify that the chart should be drawn as a scatter graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, the odometer reading ranges from 120000 to 400000, while the fuel economy ranges from 10 to 17. The multiple linear regression model will weight the odometer reading variable more heavily than the fuel economy attribute due to its higher values. However, it does not mean that the odometer reading attribute is more important as a predictor. To solve this problem, we have to normalize the values of both variables. ❤️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(df_cars) + aes(x ='odometer_reading' , y = 'fuel_economy') + geom_point(color = \"blue\" , size = 8) + theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The maximum absolute scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum absolute scaling rescales each feature between -1 and 1 by dividing every observation by its maximum absolute value.\n",
    "\n",
    "We can apply the maximum absolute scaling in Pandas using the .max() and .abs() methods, as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**x_scaled = x / max(|x|)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# apply the maximum absolute scaling in Pandas using the .abs() and .max() methods\n",
    "\n",
    "def maximum_absolute_scaling(df):\n",
    "    # copy the dataframe\n",
    "    df_scaled = df.copy()\n",
    "    # apply maximum absolute scaling\n",
    "    for column in df_scaled.columns:\n",
    "        df_scaled[column] = df_scaled[column]  / df_scaled[column].abs().max()\n",
    "    return df_scaled\n",
    "    \n",
    "\n",
    "# call the maximum_absolute_scaling function\n",
    "df_cars_scaled = maximum_absolute_scaling(df_cars)\n",
    "\n",
    "df_cars_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Alternatively, we can use the Scikit-learn library to compute the maximum absolute scaling: \n",
    "1. First, we create an abs_scaler with the MaxAbsScaler class. \n",
    "2. Then, we use the fit method to learn the required parameters for scaling the data (the maximum absolute value of each feature). \n",
    "3. Finally, we transform the data using those parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "# create an abs_scaler object\n",
    "abs_scaler = MaxAbsScaler()\n",
    "\n",
    "# calculate the maximum absolute value for scaling the data using the fit method\n",
    "# transform the data using the parameters calculated by the fit method (the maximum absolute values)\n",
    "scaled_data = abs_scaler.fit_transform(df_cars)\n",
    "\n",
    "# store the results in a data frame\n",
    "df_scaled = pd.DataFrame(scaled_data, columns=df_cars.columns)\n",
    "\n",
    "# visualize the scaled data frame\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The min-max feature scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **min-max** approach (often called **normalization**) rescales the feature to a fixed range of [0,1] by subtracting the minimum value of the feature and then dividing by the range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**x_norm = (x - x_min) / (x_max - x_min)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the min-max scaling in Pandas using the .min() and .max() methods\n",
    "def min_max_scaling(df):\n",
    "    # copy the dataframe\n",
    "    df_norm = df.copy()\n",
    "    # apply min-max scaling\n",
    "    for column in df_norm.columns:\n",
    "        df_norm[column] = (df_norm[column] - df_norm[column].min()) / (df_norm[column].max() - df_norm[column].min())\n",
    "        \n",
    "    return df_norm\n",
    "    \n",
    "# call the min_max_scaling function\n",
    "df_cars_normalized = min_max_scaling(df_cars)\n",
    "\n",
    "df_cars_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternatively**, we can use the **MinMaxScaler** class available in the **Scikit-learn** library:\n",
    "1. First, we create a **scaler** object. \n",
    "2. Then, we **fit** the **scaler** parameters, meaning we calculate the minimum and maximum value for each feature. \n",
    "3. Finally, we **transform** the data using those parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# create a scaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit and transform the data\n",
    "normad_data = scaler.fit_transform(df_cars)\n",
    "\n",
    "# store the results in a data frame\n",
    "df_norm = pd.DataFrame(normad_data, columns=df_cars.columns)\n",
    "\n",
    "# visualize the scaled data frame\n",
    "df_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the data after applying the **min-max feature scaling**. As we can observe, this normalization technique rescales all feature values to be within the range of **[0, 1]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(df_norm) + aes(x ='odometer_reading' , y = 'fuel_economy' ) + geom_point() + theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note :** \n",
    "Furthermore, it is important to bear in mind that the **maximum absolute scaling** and the **min-max scaling** are very sensitive to **outliers** because a single outlier can influence the minimum and maximum values and have a **big effect** on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The z-score method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **z-score** method (often called **standardization**) **transforms the data into a distribution with a mean of 0 and a standard deviation of 1**. Each standardized value is computed by subtracting the mean of the corresponding feature and then dividing by the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike **min-max scaling**, the **z-score** does not rescale the feature to a fixed range. The **z-score** typically ranges from -3.00 to 3.00 (more than 99% of the data) if the input is **normally distributed**. However, the standardized values can also be higher or lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to bear in mind that **z-scores** are not necessarily normally distributed. They just scale the data and follow the same distribution as the original input. This transformed distribution has a mean of 0 and a standard deviation of 1 and is going to be the **standard normal** distribution (see the image above) only if the input feature follows a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**x_std = ( x - mean ) / standard deviation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the z-score method in Pandas using the .mean() and .std() methods\n",
    "def z_score(df):\n",
    "    # copy the dataframe\n",
    "    df_std = df.copy()\n",
    "    # apply the z-score method\n",
    "    for column in df_std.columns:\n",
    "        df_std[column] = (df_std[column] - df_std[column].mean()) / df_std[column].std()\n",
    "        \n",
    "    return df_std\n",
    "    \n",
    "# call the z_score function\n",
    "df_cars_standardized = z_score(df_cars)\n",
    "\n",
    "# visualize the scaled data frame\n",
    "df_cars_standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternatively**,we can use the **StandardScaler** class available in the **Scikit-learn** library to perform the **z-score :**\n",
    "1. First, we create a **standard_scaler object**. \n",
    "2. we calculate the parameters of the transformation (in this case the mean and the standard deviation) using the **.fit()** method. \n",
    "3. we call the **.transform()** method to apply the **standardization** to the data frame. The **.transform()** method uses the parameters generated from the **.fit()** method to perform the **z-score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create a scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit and transform the data\n",
    "data = scaler.fit_transform(df_cars)\n",
    "\n",
    "# store the results in a data frame\n",
    "df_std = pd.DataFrame(data , columns = df_cars.columns)\n",
    "\n",
    "# visualize the scaled data frame\n",
    "df_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As we can observe, the results differ from those obtained using **Pandas**. The **StandardScaler** function calculates the standard deviation where the sum of squares is divided by **n** (sample size).\n",
    "2. **On the contrary**, the **.std()** method calculates the standard deviation where the denominator of the formula is **n-1** instead of **n**.\n",
    "3. To obtain the same results with **Pandas**, we set the parameter **ddof** equal to **0** (default value is **ddof=1**) which represents the divisor used in the calculations **(n-ddof)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars.std(ddof = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain the parameters calculated by the **fit** function for **standardizing** the data with the **mean_** and **scale_** attributes. As we can observe, we obtain the same results in **Scikit-learn** and **Pandas** when setting the parameter **ddof** equals to **0** in the **.std()** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation for standardizing the data\n",
    "scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean for standardizing the data\n",
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The Robust Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **robust scaling**, we scale each feature of the data set by subtracting the **median** and then dividing by the **interquartile range**. The **interquartile range (IQR)** is defined as the difference between the third and the first **quartile** and represents the central 50% of the data. Mathematically the robust scaler can be expressed as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**x_rs = (x - median ) / IQR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method comes in handy when working with data sets that contain many **outliers** because it uses statistics that are robust to **outliers** (**median** and **interquartile range**), in contrast with the previous scalers, which use statistics that are highly affected by **outliers** such as the **maximum**, the **minimum**, the **mean**, and the **standard deviation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we previously did, we can perform **robust scaling** using **Pandas** to the **cars** dataset : \n",
    "1. The **median** is defined as the midpoint of the distribution, meaning 50% of the values of the distribution are smaller than the **median.** \n",
    "2. In **Pandas**, we can calculate it with the **.median()** or the **.quantile(0.5)** methods. The **first quartile** is the **median** of the lower half of the data set (25% of the values lie below the first quartile) and can be calculated with the **.quantile(0.25)** method. The **third quartile** represents the median of the upper half of the data set (75% of the values lie below the third quartile) and can be calculated with the **.quantile(0.75)** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the robust_scaler function \n",
    "def robust_scaler(df):\n",
    "    df_rs = df.copy()\n",
    "    \n",
    "    for column in df_rs.columns :\n",
    "        df_rs[column] = ( df_rs[column] - df_rs[column].median() ) / ( df_rs[column].quantile(0.75) - df_rs[column].quantile(0.25))\n",
    "    \n",
    "    return(df_rs)\n",
    "\n",
    "# call the robust_scaler function\n",
    "df_cars_rs = robust_scaler(df_cars)\n",
    "\n",
    "# visualize the scaled data frame\n",
    "df_cars_rs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an **alternative** to **Pandas**, we can also perform **robust scaling** using the **Scikit-learn** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "robust_scaler = RobustScaler()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# create a scaler object\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# fit and transform the data\n",
    "data = scaler.fit_transform(df_cars)\n",
    "\n",
    "# store the results in a data frame\n",
    "df_rs = pd.DataFrame(data , columns = df_cars.columns)\n",
    "\n",
    "# visualize the scaled data frame\n",
    "df_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import ggplot , aes , geom_point , theme_minimal , labs\n",
    "\n",
    "ggplot(df_rs) + aes( x = 'odometer_reading', y ='fuel_economy') + geom_point(color = \"blue\" , size =8) + labs(title = \"scatter plot after RobustScaling\") + theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Between MinMaxScalr and RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see how **outliers** affect the results after scaling the data with **min-max scaling** and **robust scaling**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following data set contains 10 data points, one of them being an outlier (variable1 = 30).\n",
    "\n",
    "df_data = pd.DataFrame({'variable1':[1,2,3,4,5,6,7,30], 'variable2':[1,2,3,4,5,6,7,8]})\n",
    "\n",
    "df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaling The dataset df_data with the MinMaxScaler :**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The min-max scaling shifts the variable 1 towards 0 due to the presence of an outlier as compared with variable 2 where the points are evenly distributed in a range from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# create a scaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit and transform the data\n",
    "data = scaler.fit_transform(df_data)\n",
    "\n",
    "# store the results in a data frame\n",
    "df_norm = pd.DataFrame(data , columns = df_data.columns)\n",
    "\n",
    "# visualize the scaled data frame\n",
    "df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import ggplot , aes , geom_point , theme_minimal , labs\n",
    "\n",
    "ggplot(df_norm) + aes( x = 'variable1', y ='variable2') + geom_point() + labs(title = \"scatter plot after Min_Max_Scaling\") + theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Before scaling, the first data point has a value of (1,1), both variable 1 and variable 2 have equal values. Once transformed, the value of variable 2 is much larger than variable 1 (0.034,0.142). This is because variable 1 has an **outlier**.\n",
    "2. On the contrary, if we apply **robust scaling**, both variables have the same values (-1.00,-1.00) after the transformation, because both features have the same **median** and **interquartile range**, being the **outlier** the value that is shifted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data normalization consists of transforming numeric columns to a common scale. In Python, we can implement data normalization in a very simple way. The Pandas library contains multiple built-in methods for calculating the most common descriptive statistical functions which make data normalization techniques really easy to implement. As another option, we can use the Scikit-Learn library to transform the data into a common scale. In this library, the most frequent scaling methods are already implemented.\n",
    "Besides data normalization, there are multiple data pre-processing techniques we have to apply to guarantee the performance of the learning algorithm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines work by allowing for a linear sequence of data transforms to be chained together culminating in a modeling process that can be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = make_classification(random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
